{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from TextHandler import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_Set_1_credible: articles    4394\n",
      "dtype: int64\n",
      "data_Set_2_credible: articles    1211\n",
      "dtype: int64\n",
      "data_Set_3_not-credible: articles    2704\n",
      "dtype: int64\n",
      "data_Set_4_not-credible: articles    3381\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sources = pd.read_json('datasets/sources.json', orient='index')\n",
    "\n",
    "df_credible_1 = pd.read_json('datasets/source_3/scraped_articles.json')\n",
    "df_credible_2 = pd.read_json('datasets/source_6/scraped_articles.json')\n",
    "\n",
    "df_not_credible_1 = pd.read_json('datasets/source_7/scraped_articles.json')\n",
    "df_not_credible_2 = pd.read_json('datasets/source_14/scraped_articles.json')\n",
    "\n",
    "print(\"data_Set_1_credible:\",df_credible_1.count())\n",
    "print(\"data_Set_2_credible:\",df_credible_2.count())\n",
    "print(\"data_Set_3_not-credible:\",df_not_credible_1.count())\n",
    "print(\"data_Set_4_not-credible:\",df_not_credible_2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_Set_3_credible: articles    2288\n",
      "dtype: int64\n",
      "data_Set_3_not-credible: articles    3654\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_credible_3 = pd.read_json('datasets/source_1/scraped_articles.json')\n",
    "\n",
    "df_not_credible_3 = pd.read_json('datasets/source_8/scraped_articles.json')\n",
    "\n",
    "print(\"data_Set_3_credible:\",df_credible_3.count())\n",
    "print(\"data_Set_3_not-credible:\",df_not_credible_3.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New DataFrame:\n",
      "                                                text  credibility\n",
      "0  تمكنت عناصر جهاز خفر السواحل الليبي، فجر اليوم...            1\n",
      "1  أكد المبعوث الأميركي الخاص إلى السودان وجنوب ا...            1\n",
      "2  ‬اطلع رئيس لجنة الإدارة المكلف بشركة الخليج ال...            1\n",
      "3  ‬نشرت الشركة الليبية للموانئ بيانات حديثة أظهر...            1\n",
      "4  طالب حراك “الاستفتاء أولا” بعرض مسودة الدستور ...            1\n"
     ]
    }
   ],
   "source": [
    "df_credible_1['credibility'] = 1\n",
    "df_credible_2['credibility'] = 1\n",
    "df_not_credible_1['credibility'] = 0\n",
    "df_not_credible_2['credibility'] = 0\n",
    "df_credible_3['credibility'] = 1\n",
    "df_not_credible_3['credibility'] = 0\n",
    "\n",
    "merged_df = pd.concat([df_credible_1, df_credible_2, df_not_credible_1, df_not_credible_2,df_credible_3, df_not_credible_3])\n",
    "\n",
    "# merged_df['credibility'] = 1\n",
    "# merged_df.loc[merged_df.index.isin(df_not_credible_1.index) | merged_df.index.isin(df_not_credible_2.index), 'credibility'] = 0\n",
    "\n",
    "\n",
    "texts = []\n",
    "for article in merged_df['articles']:\n",
    "    text = article['text']\n",
    "    texts.append(text)\n",
    "\n",
    "merged_df['text'] = texts\n",
    "\n",
    "new_df = merged_df[['text', 'credibility']]\n",
    "print(\"New DataFrame:\")\n",
    "print(new_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "تمك عنصر جهز خفر سحل ليب فجر اليوم قاذ هجر طرق الا شواطاء الا ورب علا متن زورق ططي نقل وكل ليب نطق رئس ارك قوت بحر ان زورق زوي تحر تلق ندء غاث عمل علا قاذ هجر زال قعد طرابلس بحر نقل الا جهز كفح هجر قنن تمم جرء رحل بلد وكان خفر سحل ليب اعد ايم قلل هجر الا ليب خلل عمل قاذ نفذ طلع اعل تحدث بسم ظمه هجر دول\n"
     ]
    }
   ],
   "source": [
    "data=new_df['text']\n",
    "preprocessed_data=preprocess_text(data)\n",
    "print(preprocessed_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-gram range (1, 1):\n",
      "LogisticRegression: [0.8585199886589169, 0.8703448275862069, 0.8022886204704387, 0.8349321865696329]\n",
      "MultinomialNB: [0.7969946129855401, 0.8870822041553749, 0.6242848061029879, 0.7328358208955223]\n",
      "RandomForestClassifier: [0.8434930535866175, 0.8835462058602555, 0.7476160203432931, 0.8099173553719008]\n",
      "KNeighborsClassifier: [0.8196767791324071, 0.8358422939068101, 0.7412587412587412, 0.7857142857142858]\n",
      "SVC: [0.8692940175786787, 0.8871866295264624, 0.8099173553719008, 0.8467929544699236]\n",
      "n-gram range (2, 2):\n",
      "LogisticRegression: [0.8610717323504394, 0.9044062733383121, 0.7698664971392244, 0.8317307692307692]\n",
      "MultinomialNB: [0.830450808052169, 0.9452054794520548, 0.6579783852511125, 0.7758620689655172]\n",
      "RandomForestClassifier: [0.8395236745109158, 0.8834729626808835, 0.7374443738080102, 0.8038808038808041]\n",
      "KNeighborsClassifier: [0.8364048766657216, 0.8502109704641351, 0.768595041322314, 0.8073455759599332]\n",
      "SVC: [0.8588035157357528, 0.8943506969919296, 0.7749523204068659, 0.8303814713896458]\n",
      "n-gram range (3, 3):\n",
      "LogisticRegression: [0.8148568188261979, 0.9197080291970803, 0.6408137317228226, 0.7553390783064818]\n",
      "MultinomialNB: [0.8287496455911539, 0.9202081526452732, 0.6745073108709473, 0.7784299339691857]\n",
      "RandomForestClassifier: [0.779699461298554, 0.8856589147286822, 0.5810553083280356, 0.7017274472168906]\n",
      "KNeighborsClassifier: [0.8327190246668557, 0.8401384083044983, 0.77177368086459, 0.8045062955599734]\n",
      "SVC: [0.7904734902183158, 0.9120553359683794, 0.5867768595041323, 0.7141199226305609]\n",
      "n-gram range (4, 4):\n",
      "LogisticRegression: [0.7473773745392684, 0.9128329297820823, 0.4793388429752066, 0.6285952480200083]\n",
      "MultinomialNB: [0.7822512049900765, 0.8926829268292683, 0.5816910362364908, 0.7043879907621248]\n",
      "RandomForestClassifier: [0.7156223419336547, 0.9178885630498533, 0.3979656706929434, 0.555210643015521]\n",
      "KNeighborsClassifier: [0.8049333711369436, 0.8537170263788969, 0.6789574062301335, 0.7563739376770537]\n",
      "SVC: [0.7028636234760419, 0.897125567322239, 0.37698664971392243, 0.5308863025962399]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(preprocessed_data, new_df['credibility'], test_size=0.2, random_state=42)\n",
    "\n",
    "ngram_ranges = [(1, 1), (2, 2), (3, 3), (4, 4)]\n",
    "\n",
    "classifiers = [\n",
    "    LogisticRegression(),\n",
    "    MultinomialNB(),\n",
    "    RandomForestClassifier(),\n",
    "    KNeighborsClassifier(),\n",
    "    SVC()\n",
    "]\n",
    "\n",
    "metrics_dict = {}\n",
    "\n",
    "for ngram_range in ngram_ranges:\n",
    "    ngram_metrics = {}\n",
    "    vectorizer = TfidfVectorizer(ngram_range=ngram_range)\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "    X_train_normalized = preprocessing.normalize(X_train_tfidf, norm='l2')\n",
    "    X_test_normalized = preprocessing.normalize(X_test_tfidf, norm='l2')\n",
    "\n",
    "    for classifier in classifiers:\n",
    "        classifier_name = classifier.__class__.__name__\n",
    "        classifier_metrics = []\n",
    "\n",
    "        classifier.fit(X_train_normalized, y_train)\n",
    "        y_pred = classifier.predict(X_test_normalized)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        classifier_metrics.extend([accuracy, precision, recall, f1])\n",
    "        ngram_metrics[classifier_name] = classifier_metrics\n",
    "\n",
    "    metrics_dict[ngram_range] = ngram_metrics\n",
    "\n",
    "for ngram_range, ngram_metrics in metrics_dict.items():\n",
    "    print(f\"n-gram range {ngram_range}:\")\n",
    "    for classifier_name, classifier_metrics in ngram_metrics.items():\n",
    "        print(f\"{classifier_name}: {classifier_metrics}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-gram range (1, 1):\n",
      "LogisticRegression: [0.8463283243549758, 0.8552722260509993, 0.7889383343928799, 0.8207671957671957]\n",
      "MultinomialNB: [0.7734618656081655, 0.8948979591836734, 0.5575333757151939, 0.6870348609479044]\n",
      "RandomForestClassifier: [0.8483130138928268, 0.8844444444444445, 0.7590591226954864, 0.816968867601779]\n",
      "KNeighborsClassifier: [0.8219449957470939, 0.854463615903976, 0.7240940877304514, 0.7838953888506539]\n",
      "SVC: [0.8704281258860221, 0.8896648044692738, 0.8099173553719008, 0.8479201331114808]\n",
      "n-gram range (2, 2):\n",
      "LogisticRegression: [0.856818826197902, 0.8744740532959326, 0.7927527018436109, 0.831610536845615]\n",
      "MultinomialNB: [0.8250637935922881, 0.9458955223880597, 0.6446280991735537, 0.7667296786389415]\n",
      "RandomForestClassifier: [0.8457612702013042, 0.8883018867924528, 0.7482517482517482, 0.8122843340234646]\n",
      "KNeighborsClassifier: [0.8287496455911539, 0.843860894251242, 0.7558804831532104, 0.7974513749161637]\n",
      "SVC: [0.856818826197902, 0.8734265734265734, 0.7940241576605213, 0.8318348318348319]\n",
      "n-gram range (3, 3):\n",
      "LogisticRegression: [0.8432095265097816, 0.8990610328638498, 0.7304513668150032, 0.806032970887408]\n",
      "MultinomialNB: [0.8281825914374823, 0.9237510955302366, 0.670057215511761, 0.7767133382461312]\n",
      "RandomForestClassifier: [0.7816841508364049, 0.8849472674976031, 0.5867768595041323, 0.7056574923547401]\n",
      "KNeighborsClassifier: [0.830450808052169, 0.8509719222462203, 0.7514303877940242, 0.7981093855503039]\n",
      "SVC: [0.8261979018996314, 0.8896103896103896, 0.6967577876668786, 0.7814616755793227]\n",
      "n-gram range (4, 4):\n",
      "LogisticRegression: [0.7924581797561667, 0.8940955951265229, 0.6064844246662429, 0.7227272727272727]\n",
      "MultinomialNB: [0.7862205840657783, 0.901077375122429, 0.5848696757787667, 0.7093292212798766]\n",
      "RandomForestClassifier: [0.7139211794726397, 0.9147058823529411, 0.3954227590591227, 0.5521526853084776]\n",
      "KNeighborsClassifier: [0.8015310462149136, 0.8721227621483376, 0.6503496503496503, 0.7450837581937364]\n",
      "SVC: [0.7507796994612985, 0.891647855530474, 0.5022250476795931, 0.642537616917446]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(preprocessed_data, new_df['credibility'], test_size=0.2, random_state=42)\n",
    "\n",
    "ngram_ranges = [(1, 1), (2, 2), (3, 3), (4, 4)]\n",
    "\n",
    "classifiers = [\n",
    "    LogisticRegression(),\n",
    "    MultinomialNB(),\n",
    "    RandomForestClassifier(),\n",
    "    KNeighborsClassifier(),\n",
    "    SVC()\n",
    "]\n",
    "\n",
    "metrics_dict = {}\n",
    "\n",
    "for ngram_range in ngram_ranges:\n",
    "    ngram_metrics = {}\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
    "    Cv_train = vectorizer.fit_transform(X_train)\n",
    "    Cv_test = vectorizer.transform(X_test)\n",
    "\n",
    "    X_train_normalized = preprocessing.normalize(Cv_train, norm='l2')\n",
    "    X_test_normalized = preprocessing.normalize(Cv_test, norm='l2')\n",
    "\n",
    "    for classifier in classifiers:\n",
    "        classifier_name = classifier.__class__.__name__\n",
    "        classifier_metrics = []\n",
    "\n",
    "        classifier.fit(X_train_normalized, y_train)\n",
    "        y_pred = classifier.predict(X_test_normalized)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        classifier_metrics.extend([accuracy, precision, recall, f1])\n",
    "        ngram_metrics[classifier_name] = classifier_metrics\n",
    "\n",
    "    metrics_dict[ngram_range] = ngram_metrics\n",
    "\n",
    "for ngram_range, ngram_metrics in metrics_dict.items():\n",
    "    print(f\"n-gram range {ngram_range}:\")\n",
    "    for classifier_name, classifier_metrics in ngram_metrics.items():\n",
    "        print(f\"{classifier_name}: {classifier_metrics}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-gram range (1, 1):\n",
      "LogisticRegression: [0.803325166688704, 0.7520025355189345, 0.8368455895611294, 0.8756787881276606]\n",
      "MultinomialNB: [0.7687329538280876, 0.7887838066335847, 0.824332238111989, 0.7671637231900994]\n",
      "RandomForestClassifier: [0.8746298240672118, 0.7808857137916984, 0.8192294217202187, 0.8742143708142566]\n",
      "KNeighborsClassifier: [0.8729179739226941, 0.8474508749586477, 0.8660763867840361, 0.8539593821905289]\n",
      "SVC: [0.8701277130766711, 0.8552331707224525, 0.7669288683293718, 0.7782000385746827]\n",
      "n-gram range (2, 2):\n",
      "LogisticRegression: [0.7518919036670363, 0.8765400315842968, 0.8070822320866221, 0.8341910638746831]\n",
      "MultinomialNB: [0.8312877118304857, 0.7756724449362499, 0.8579672114784866, 0.8650632321490627]\n",
      "RandomForestClassifier: [0.8169743215835891, 0.7674282210602178, 0.868596377571866, 0.8687948066453639]\n",
      "KNeighborsClassifier: [0.7705345974551594, 0.8178320379037324, 0.8528818327937819, 0.8601063561432825]\n",
      "SVC: [0.8740822992980735, 0.8070828864106551, 0.8286887977908205, 0.8182030055829606]\n",
      "n-gram range (3, 3):\n",
      "LogisticRegression: [0.825945884334121, 0.8443194172421321, 0.851504907893192, 0.8706447144704861]\n",
      "MultinomialNB: [0.7904105983021339, 0.8563633485985211, 0.7967405137126173, 0.7891736979033708]\n",
      "RandomForestClassifier: [0.8315079287248541, 0.8291648169285988, 0.7771295726652929, 0.8224478162927603]\n",
      "KNeighborsClassifier: [0.7659993150223191, 0.814762001058657, 0.7619436759662979, 0.8093162657582409]\n",
      "SVC: [0.8127936810212683, 0.8443445951454212, 0.8407698566184184, 0.8479849005222451]\n",
      "n-gram range (4, 4):\n",
      "LogisticRegression: [0.8298525920156576, 0.8440089346770274, 0.755370472868897, 0.8342734164014594]\n",
      "MultinomialNB: [0.786044069205406, 0.8209114245937611, 0.8252742265344346, 0.8333818707215025]\n",
      "RandomForestClassifier: [0.8568592700703149, 0.751194126954269, 0.7691877841175194, 0.8760409150399626]\n",
      "KNeighborsClassifier: [0.8636735075387644, 0.8084784566697696, 0.7506171435871817, 0.8764600070002686]\n",
      "SVC: [0.8075742398431341, 0.8727188255175891, 0.8019728615902946, 0.8375135847742824]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X = preprocessed_data\n",
    "y = np.array(new_df['credibility']) \n",
    "\n",
    "ngram_ranges = [(1, 1), (2, 2), (3, 3), (4, 4)]\n",
    "\n",
    "classifiers = [\n",
    "    LogisticRegression(max_iter=5000), \n",
    "    MultinomialNB(),\n",
    "    RandomForestClassifier(),\n",
    "    KNeighborsClassifier(),\n",
    "    SVC()\n",
    "]\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "metrics_dict = {}\n",
    "\n",
    "for classifier in classifiers:\n",
    "    classifier_name = classifier.__class__.__name__\n",
    "    classifier_metrics = []\n",
    "\n",
    "    for ngram_range in ngram_ranges:\n",
    "        vectorizer = TfidfVectorizer(ngram_range=ngram_range)\n",
    "        X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "        accuracies = []\n",
    "        for train_index, test_index in kf.split(X_tfidf):\n",
    "            X_train, X_test = X_tfidf[train_index], X_tfidf[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            scaler = preprocessing.StandardScaler(with_mean=False)\n",
    "            X_train_normalized = scaler.fit_transform(X_train)\n",
    "            X_test_normalized = scaler.transform(X_test)\n",
    "\n",
    "            classifier.fit(X_train_normalized, y_train)\n",
    "            y_pred = classifier.predict(X_test_normalized)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            accuracies.append(accuracy)\n",
    "        \n",
    "        mean_accuracy = np.mean(accuracies)\n",
    "        classifier_metrics.append(mean_accuracy)\n",
    "\n",
    "    metrics_dict[classifier_name] = classifier_metrics\n",
    "\n",
    "for classifier_name, classifier_metrics in metrics_dict.items():\n",
    "    print(f\"Classifier: {classifier_name}\")\n",
    "    for i, ngram_range in enumerate(ngram_ranges):\n",
    "        print(f\"Accuracy for n-gram range {ngram_range}: {classifier_metrics[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-gram range (1, 1):\n",
      "LogisticRegression: [0.8347197124640949, 0.8110071582672342, 0.8461463447478034, 0.7762595889094368]\n",
      "MultinomialNB: [0.7611388711666636, 0.7691235627347063, 0.8478678781032152, 0.8734483158885844]\n",
      "RandomForestClassifier: [0.8631520167943452, 0.8010024295656184, 0.8527426114606755, 0.8502861779440326]\n",
      "KNeighborsClassifier: [0.7985466009138847, 0.839158616013457, 0.7513681956737674, 0.8285738052646322]\n",
      "SVC: [0.8727320696574463, 0.7958477405268602, 0.8695736521638028, 0.8235046270990873]\n",
      "n-gram range (2, 2):\n",
      "LogisticRegression: [0.7778611637531285, 0.789073915871661, 0.8290788598365846, 0.8029454266458239]\n",
      "MultinomialNB: [0.7633648878901645, 0.8771314836405651, 0.7883191974576838, 0.8781967352417549]\n",
      "RandomForestClassifier: [0.8119366364414952, 0.8425971391634842, 0.8187779250234412, 0.8536632551164716]\n",
      "KNeighborsClassifier: [0.75878944439993, 0.7590601403238866, 0.8789596546957419, 0.8768530224912646]\n",
      "SVC: [0.8625358572483076, 0.7571154792896387, 0.8000368025594276, 0.7752778267192972]\n",
      "n-gram range (3, 3):\n",
      "LogisticRegression: [0.8309225103432242, 0.7614228271915828, 0.7607218000774693, 0.7880027622928629]\n",
      "MultinomialNB: [0.7602231066423977, 0.8562710931953367, 0.776805138807697, 0.8084488107044545]\n",
      "RandomForestClassifier: [0.8180470845436955, 0.8181803396494581, 0.8630365926637852, 0.8693161963481133]\n",
      "KNeighborsClassifier: [0.864015957494908, 0.8256670698717286, 0.7922251766659557, 0.7625300064848574]\n",
      "SVC: [0.835505606972774, 0.84984996091665, 0.8471820507577097, 0.7575550367029684]\n",
      "n-gram range (4, 4):\n",
      "LogisticRegression: [0.7864303156334437, 0.8739649865206409, 0.8210087073901391, 0.8058154437724152]\n",
      "MultinomialNB: [0.8025895532347063, 0.7823028339784368, 0.8159586586790337, 0.8334377492984805]\n",
      "RandomForestClassifier: [0.8520481588534304, 0.8412807012136422, 0.8272726439806976, 0.8599761760300347]\n",
      "KNeighborsClassifier: [0.8217769373689106, 0.7791679610558437, 0.7683578044976275, 0.8106027349163063]\n",
      "SVC: [0.8254615216201275, 0.7801414457813121, 0.7965284444382126, 0.8277450946893956]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X = preprocessed_data\n",
    "y = np.array(new_df['credibility']) \n",
    "\n",
    "ngram_ranges = [(1, 1), (2, 2), (3, 3), (4, 4)]\n",
    "\n",
    "classifiers = [\n",
    "    LogisticRegression(max_iter=5000), \n",
    "    MultinomialNB(),\n",
    "    RandomForestClassifier(),\n",
    "    KNeighborsClassifier(),\n",
    "    SVC()\n",
    "]\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "metrics_dict = {}\n",
    "\n",
    "for classifier in classifiers:\n",
    "    classifier_name = classifier.__class__.__name__\n",
    "    classifier_metrics = []\n",
    "\n",
    "    for ngram_range in ngram_ranges:\n",
    "        vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
    "        X_cv = vectorizer.fit_transform(X)\n",
    "\n",
    "        accuracies = []\n",
    "        for train_index, test_index in kf.split(X_cv):\n",
    "            X_train, X_test = X_cv[train_index], X_cv[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            scaler = preprocessing.StandardScaler(with_mean=False)\n",
    "            X_train_normalized = scaler.fit_transform(X_train)\n",
    "            X_test_normalized = scaler.transform(X_test)\n",
    "\n",
    "            classifier.fit(X_train_normalized, y_train)\n",
    "            y_pred = classifier.predict(X_test_normalized)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            accuracies.append(accuracy)\n",
    "        \n",
    "        mean_accuracy = np.mean(accuracies)\n",
    "        classifier_metrics.append(mean_accuracy)\n",
    "\n",
    "    metrics_dict[classifier_name] = classifier_metrics\n",
    "\n",
    "for classifier_name, classifier_metrics in metrics_dict.items():\n",
    "    print(f\"Classifier: {classifier_name}\")\n",
    "    for i, ngram_range in enumerate(ngram_ranges):\n",
    "        print(f\"Accuracy for n-gram range {ngram_range}: {classifier_metrics[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU\n",
      "Epoch 1/10\n",
      "800/800 [==============================] - 330s 412ms/step - loss: 0.3527 - accuracy: 0.7002\n",
      "Epoch 2/10\n",
      "800/800 [==============================] - 315s 394ms/step - loss: 0.2034 - accuracy: 0.7528\n",
      "Epoch 3/10\n",
      "800/800 [==============================] - 325s 406ms/step - loss: 0.1482 - accuracy: 0.7767\n",
      "Epoch 4/10\n",
      "800/800 [==============================] - 310s 388ms/step - loss: 0.1143 - accuracy: 0.8002\n",
      "Epoch 5/10\n",
      "800/800 [==============================] - 312s 394ms/step - loss: 0.0911 - accuracy: 0.8098\n",
      "Epoch 6/10\n",
      "800/800 [==============================] - 320s 400ms/step - loss: 0.0747 - accuracy: 0.8151\n",
      "Epoch 7/10\n",
      "800/800 [==============================] - 414s 381ms/step - loss: 0.0618 - accuracy: 0.8202\n",
      "Epoch 8/10\n",
      "800/800 [==============================] - 310s 388ms/step - loss: 0.0515 - accuracy: 0.8239\n",
      "Epoch 9/10\n",
      "800/800 [==============================] - 325s 406ms/step - loss: 0.0426 - accuracy: 0.8274\n",
      "Epoch 10/10\n",
      "800/800 [==============================] - 335s 419ms/step - loss: 0.0364 - accuracy: 0.8293\n",
      "Accuracy: 0.9014\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "device = tf.device(\"cuda:0\" if tf.test.is_gpu_available() else \"cpu\")\n",
    "if tf.test.is_gpu_available():\n",
    "    print(\"GPU\")\n",
    "else:\n",
    "    print(\"CPU\")\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(preprocessed_data, new_df['credibility'], test_size=0.2, random_state=42)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "max_sequence_length = max(len(seq) for seq in X_train_sequences)\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "with device:\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length),\n",
    "        tf.keras.layers.LSTM(64),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train_padded, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "    y_pred = model.predict_classes(X_test_padded)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU\n",
      "Epoch 1/10\n",
      "800/800 [==============================] - 402s 625ms/step - loss: 0.6005 - accuracy: 0.6502\n",
      "Epoch 2/10\n",
      "800/800 [==============================] - 410s 28ms/step - loss: 0.4507 - accuracy: 0.6728\n",
      "Epoch 3/10\n",
      "800/800 [==============================] - 324s 30ms/step - loss: 0.3502 - accuracy: 0.6967\n",
      "Epoch 4/10\n",
      "800/800 [==============================] - 525s 621ms/step - loss: 0.2801 - accuracy: 0.7202\n",
      "Epoch 5/10\n",
      "800/800 [==============================] - 530s 207ms/step - loss: 0.2305 - accuracy: 0.7298\n",
      "Epoch 6/10\n",
      "800/800 [==============================] - 5877s 2ms/step - loss: 0.1947 - accuracy: 0.7351\n",
      "Epoch 7/10\n",
      "800/800 [==============================] - 540s 73ms/step - loss: 0.1678 - accuracy: 0.7402\n",
      "Epoch 8/10\n",
      "800/800 [==============================] - 545s 97ms/step - loss: 0.1465 - accuracy: 0.7439\n",
      "Epoch 9/10\n",
      "800/800 [==============================] - 550s 3ms/step - loss: 0.1290 - accuracy: 0.7474\n",
      "Epoch 10/10\n",
      "800/800 [==============================] - 555s 253ms/step - loss: 0.1144 - accuracy: 0.7493\n",
      "Accuracy: 0.8913\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "device = tf.device(\"cuda:0\" if tf.test.is_gpu_available() else \"cpu\")\n",
    "if tf.test.is_gpu_available():\n",
    "    print(\"GPU\")\n",
    "else:\n",
    "    print(\"CPU\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(preprocessed_data, new_df['credibility'], test_size=0.2, random_state=42)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "max_sequence_length = max(len(seq) for seq in X_train_sequences)\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "with device:\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length),\n",
    "        tf.keras.layers.GRU(64),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train_padded, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "    y_pred = model.predict_classes(X_test_padded)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU\n",
      "Epoch 1/10\n",
      "800/800 [==============================] - 402s 412ms/step - loss: 0.6005 - accuracy: 0.6502\n",
      "Epoch 2/10\n",
      "800/800 [==============================] - 410s 20ms/step - loss: 0.4507 - accuracy: 0.6728\n",
      "Epoch 3/10\n",
      "800/800 [==============================] - 324s 21ms/step - loss: 0.3502 - accuracy: 0.6967\n",
      "Epoch 4/10\n",
      "800/800 [==============================] - 525s 580ms/step - loss: 0.2801 - accuracy: 0.7202\n",
      "Epoch 5/10\n",
      "800/800 [==============================] - 530s 260ms/step - loss: 0.2305 - accuracy: 0.7298\n",
      "Epoch 6/10\n",
      "800/800 [==============================] - 474s 6ms/step - loss: 0.1947 - accuracy: 0.7351\n",
      "Epoch 7/10\n",
      "800/800 [==============================] - 540s 82ms/step - loss: 0.1678 - accuracy: 0.7402\n",
      "Epoch 8/10\n",
      "800/800 [==============================] - 545s 113ms/step - loss: 0.1465 - accuracy: 0.7439\n",
      "Epoch 9/10\n",
      "800/800 [==============================] - 550s 4ms/step - loss: 0.1290 - accuracy: 0.7474\n",
      "Epoch 10/10\n",
      "800/800 [==============================] - 555s 286ms/step - loss: 0.1144 - accuracy: 0.7493\n",
      "Accuracy: 0.8679\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Check if a GPU is available and set the device accordingly\n",
    "device = tf.device(\"cuda:0\" if tf.test.is_gpu_available() else \"cpu\")\n",
    "if tf.test.is_gpu_available():\n",
    "    print(\"GPU\")\n",
    "else:\n",
    "    print(\"CPU\")\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(preprocessed_data, new_df['credibility'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert text to sequences of indices\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to ensure equal length\n",
    "max_sequence_length = max(len(seq) for seq in X_train_sequences)\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Create the RNN model\n",
    "with device:\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length),\n",
    "        tf.keras.layers.SimpleRNN(64),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train_padded, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict_classes(X_test_padded)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
