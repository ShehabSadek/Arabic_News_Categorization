{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from TextHandler import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'type'], dtype='object')\n",
      "Index(['Content', 'Link', 'Category'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_1= pd.read_csv('data/arabic_categorization_data.csv')\n",
    "df_1.drop(df_1.columns[0], axis=1, inplace=True)\n",
    "df_1.dropna(inplace=True)\n",
    "print(df_1.columns)\n",
    "\n",
    "df_2= pd.read_csv('data/processed_data.csv')\n",
    "df_2.drop(df_2.columns[0], axis=1, inplace=True)\n",
    "df_2.dropna(inplace=True)\n",
    "print(df_2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    \\r\\nأشرف رئيس الجمهورية الباجي قايد السبسي الي...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data=df_1['text']\n",
    "print(data[:1])\n",
    "preprocessed_data_1=preprocess_text(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amr\\AppData\\Local\\Temp\\ipykernel_21888\\3943842632.py:2: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  print(data[:1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    جاءت جوله التمويل التمهيديه الاولي للشركه بقيا...\n",
      "Name: Content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data=df_2['Content']\n",
    "print(data[:1])\n",
    "preprocessed_data_2=preprocess_text(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(preprocessed_data_1, df_1['type'], test_size=0.2, random_state=42)\n",
    "\n",
    "# ngram_ranges = [(1, 1), (2, 2), (3, 3), (4, 4)]\n",
    "\n",
    "# classifiers = [\n",
    "#     LogisticRegression(max_iter=1000),\n",
    "#     MultinomialNB(),\n",
    "#     KNeighborsClassifier(),\n",
    "#     RandomForestClassifier(),\n",
    "#     SVC()\n",
    "# ]\n",
    "\n",
    "# for classifier in classifiers:\n",
    "#     classifier_name = classifier.__class__.__name__\n",
    "#     print(f\"Classifier: {classifier_name}\")\n",
    "#     for ngram_range in ngram_ranges:\n",
    "#         vectorizer = TfidfVectorizer(ngram_range=ngram_range)\n",
    "#         X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "#         X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "#         classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "#         y_pred = classifier.predict(X_test_tfidf)\n",
    "\n",
    "#         accuracy = accuracy_score(y_test, y_pred)\n",
    "#         print(f\"Accuracy for n-gram range {ngram_range}: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: LogisticRegression\n",
      "Accuracy for n-gram range (1, 1): 0.9761620977353993\n",
      "Accuracy for n-gram range (2, 2): 0.9725864123957092\n",
      "Accuracy for n-gram range (3, 3): 0.9618593563766389\n",
      "Accuracy for n-gram range (4, 4): 0.9249106078665077\n",
      "Classifier: MultinomialNB\n",
      "Accuracy for n-gram range (1, 1): 0.9493444576877235\n",
      "Accuracy for n-gram range (2, 2): 0.9576877234803337\n",
      "Accuracy for n-gram range (3, 3): 0.9558998808104887\n",
      "Accuracy for n-gram range (4, 4): 0.9016686531585221\n",
      "Classifier: KNeighborsClassifier\n",
      "Accuracy for n-gram range (1, 1): 0.9415971394517283\n",
      "Accuracy for n-gram range (2, 2): 0.964243146603099\n",
      "Accuracy for n-gram range (3, 3): 0.9529201430274136\n",
      "Accuracy for n-gram range (4, 4): 0.8224076281287247\n",
      "Classifier: RandomForestClassifier\n",
      "Accuracy for n-gram range (1, 1): 0.9630512514898689\n",
      "Accuracy for n-gram range (2, 2): 0.9600715137067938\n",
      "Accuracy for n-gram range (3, 3): 0.9135876042908224\n",
      "Accuracy for n-gram range (4, 4): 0.799165673420739\n",
      "Classifier: SVC\n",
      "Accuracy for n-gram range (1, 1): 0.9791418355184743\n",
      "Accuracy for n-gram range (2, 2): 0.968414779499404\n",
      "Accuracy for n-gram range (3, 3): 0.935041716328963\n",
      "Accuracy for n-gram range (4, 4): 0.8480333730631704\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(preprocessed_data_2, df_2['Category'], test_size=0.2, random_state=42)\n",
    "\n",
    "ngram_ranges = [(1, 1), (2, 2), (3, 3), (4, 4)]\n",
    "\n",
    "classifiers = [\n",
    "    LogisticRegression(max_iter=1000),\n",
    "    MultinomialNB(),\n",
    "    KNeighborsClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    SVC()\n",
    "]\n",
    "\n",
    "for classifier in classifiers:\n",
    "    classifier_name = classifier.__class__.__name__\n",
    "    print(f\"Classifier: {classifier_name}\")\n",
    "    for ngram_range in ngram_ranges:\n",
    "        vectorizer = TfidfVectorizer(ngram_range=ngram_range)\n",
    "        X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "        X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "        classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "        y_pred = classifier.predict(X_test_tfidf)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Accuracy for n-gram range {ngram_range}: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amr\\AppData\\Local\\Temp\\ipykernel_21888\\952271838.py:23: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  y_train = y_train[:int(y_train.shape[0] / 3)]\n",
      "C:\\Users\\Amr\\AppData\\Local\\Temp\\ipykernel_21888\\952271838.py:24: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  y_test = y_test[:int(y_test.shape[0] / 3)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [2]\n",
      " [2]\n",
      " [0]]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 63\u001b[0m\n\u001b[0;32m     60\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mNLLLoss()\n\u001b[0;32m     61\u001b[0m logps \u001b[39m=\u001b[39m model(X_train_torch)\n\u001b[1;32m---> 63\u001b[0m loss \u001b[39m=\u001b[39m criterion(logps, y_train_torch)\n\u001b[0;32m     64\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     65\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.002\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:216\u001b[0m, in \u001b[0;36mNLLLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 216\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mnll_loss(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:2704\u001b[0m, in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2702\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2703\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 2704\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mnll_loss_nd(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import scipy.sparse\n",
    "from torch import nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(preprocessed_data_2, df_2['Category'], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2,2))\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "# turn X_train_tfidf to float32 to not take too much memory\n",
    "X_train_tfidf = X_train_tfidf.astype(np.float32)\n",
    "X_test_tfidf = X_test_tfidf.astype(np.float32)\n",
    "\n",
    "# use half the elements of the sparse matrix\n",
    "X_train_tfidf = scipy.sparse.csr_matrix(X_train_tfidf[:int(X_train_tfidf.shape[0] / 3)])\n",
    "X_test_tfidf = scipy.sparse.csr_matrix(X_test_tfidf[:int(X_test_tfidf.shape[0] / 3)])\n",
    "y_train = y_train[:int(y_train.shape[0] / 3)]\n",
    "y_test = y_test[:int(y_test.shape[0] / 3)]\n",
    "\n",
    "# check all unique text values in y_train and y_test, and assign numbers for each unique value in y_train and y_test, create a dictionary for this, then replace the values in y_train and y_test with the numbers, make sure to store the dictionary in a variable\n",
    "y_train = pd.DataFrame(y_train)\n",
    "y_test = pd.DataFrame(y_test)\n",
    "y_train['Category'] = y_train['Category'].astype('category')\n",
    "y_test['Category'] = y_test['Category'].astype('category')\n",
    "y_train_dict = dict(enumerate(y_train['Category'].cat.categories))\n",
    "y_test_dict = dict(enumerate(y_test['Category'].cat.categories))\n",
    "y_train['Category'] = y_train['Category'].cat.codes\n",
    "y_test['Category'] = y_test['Category'].cat.codes\n",
    "\n",
    "# convert to numpy arrays\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(y_train)\n",
    "# convert to torch tensors\n",
    "X_train_torch = torch.from_numpy(X_train_tfidf.todense())\n",
    "X_test_torch = torch.from_numpy(X_test_tfidf.todense())\n",
    "#convert y to torch tensor (y is text)\n",
    "y_train_torch = torch.from_numpy(y_train)\n",
    "y_test_torch = torch.from_numpy(y_test)\n",
    "\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(X_train_torch.shape[1], 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, df_2['Category'].nunique()),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "logps = model(X_train_torch)\n",
    "\n",
    "loss = criterion(logps, y_train_torch)\n",
    "loss.backward()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "\n",
    "epochs = 50\n",
    "for e in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = model.forward(X_train_torch)\n",
    "    loss = criterion(output, y_train_torch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        log_ps = model(X_test_torch)\n",
    "        test_loss = criterion(log_ps, y_test_torch)\n",
    "        ps = torch.exp(log_ps)\n",
    "        top_p, top_class = ps.topk(1, dim=1)\n",
    "        equals = top_class == y_test_torch.view(*top_class.shape)\n",
    "        test_accuracy = torch.mean(equals.float())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
