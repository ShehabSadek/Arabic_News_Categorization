{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from TextHandler import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text     type\n",
      "0  \\nأشرف رئيس الجمهورية الباجي قايد السبسي اليوم...  culture\n",
      "1  \\nتحصل كتاب \"المصحف وقراءاته\" الذي ألفه باحثون...  culture\n",
      "2  \\nاستنكرت إدارة المسرح الوطني التونسي الحملة ا...  culture\n",
      "3  \\nاحتضن جناح تونس في القرية الدولية للأفلام بم...  culture\n",
      "4  \\nشهدت برلين أمس الجمعة افتتاح مسجد فريد من نو...  culture\n",
      "                                             Content  \\\n",
      "0  جاءت جوله التمويل التمهيديه الاولي للشركه بقيا...   \n",
      "1  تعد دراسه الجدوي متطلبا اساسيا لنجاح اي مشروع ...   \n",
      "2  وقالت لجنه نوبل للمره الاولي التاريخ يمكن الان...   \n",
      "3  نشرت الوكاله وثاءق علي الانترنت اعدها موظفوها ...   \n",
      "4  دراسه حديثه قام فريق العلماء جونز هوبكنز ميديس...   \n",
      "\n",
      "                                                Link         Category  \n",
      "0  https://ryadiybusiness.com/%d8%aa%d9%8a%d8%b1%...      ريادة أعمال  \n",
      "1  https://ryadiybusiness.com/%d9%83%d9%8a%d9%81-...      ريادة أعمال  \n",
      "2  https://arabic.sputniknews.com/science/2020100...  علوم وتكنولوجيا  \n",
      "3  https://arabic.sputniknews.com/world/202012151...  علوم وتكنولوجيا  \n",
      "4  https://arabic.rt.com//technology/1161864-%D8%...  علوم وتكنولوجيا  \n"
     ]
    }
   ],
   "source": [
    "df_1= pd.read_csv('data/arabic_categorization_data.csv')\n",
    "df_1.drop(df_1.columns[0], axis=1, inplace=True)\n",
    "df_1.dropna(inplace=True)\n",
    "print(df_1.head())\n",
    "\n",
    "df_2= pd.read_csv('data/processed_data.csv')\n",
    "df_2.drop(df_2.columns[0], axis=1, inplace=True)\n",
    "df_2.dropna(inplace=True)\n",
    "print(df_2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    \\nأشرف رئيس الجمهورية الباجي قايد السبسي اليوم...\n",
      "Name: text, dtype: object\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m data\u001b[39m=\u001b[39mdf_1[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(data[:\u001b[39m1\u001b[39m])\n\u001b[1;32m----> 3\u001b[0m preprocessed_data\u001b[39m=\u001b[39mpreprocess_text(data[:\u001b[39m1\u001b[39;49m])\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(preprocessed_data)\n\u001b[0;32m      6\u001b[0m \u001b[39m# for n,text in enumerate(preprocessed_data):\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m#     for i in  range(1,5):\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m#         if(len(text.split())<i):\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m \u001b[39m#     print('')\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC\\Documents\\vscode\\Arabic_News_Categorization\\TextHandler.py:27\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(words)\u001b[0m\n\u001b[0;32m     25\u001b[0m text\u001b[39m=\u001b[39mremove_english_text(text)\n\u001b[0;32m     26\u001b[0m \u001b[39m# Tokenize the text\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m tokens \u001b[39m=\u001b[39m araby\u001b[39m.\u001b[39;49mtokenize(text)\n\u001b[0;32m     29\u001b[0m \u001b[39m# Filter out English words\u001b[39;00m\n\u001b[0;32m     30\u001b[0m arabic_tokens \u001b[39m=\u001b[39m [token \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokens \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m token\u001b[39m.\u001b[39misascii()]\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pyarabic\\araby.py:1385\u001b[0m, in \u001b[0;36mtokenize\u001b[1;34m(text, conditions, morphs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(conditions) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlist\u001b[39m: conditions \u001b[39m=\u001b[39m [conditions]\n\u001b[0;32m   1383\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(morphs) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlist\u001b[39m: morphs \u001b[39m=\u001b[39m [morphs]\n\u001b[1;32m-> 1385\u001b[0m tokens \u001b[39m=\u001b[39m TOKEN_PATTERN\u001b[39m.\u001b[39;49msplit(text)\n\u001b[0;32m   1386\u001b[0m tokens \u001b[39m=\u001b[39m [TOKEN_REPLACE\u001b[39m.\u001b[39msub(\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, tok) \u001b[39mfor\u001b[39;00m tok \u001b[39min\u001b[39;00m tokens \u001b[39mif\u001b[39;00m TOKEN_REPLACE\u001b[39m.\u001b[39msub(\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, tok)]\n\u001b[0;32m   1388\u001b[0m \u001b[39mif\u001b[39;00m conditions:\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object, got 'list'"
     ]
    }
   ],
   "source": [
    "data=df_1['text']\n",
    "print(data[:1])\n",
    "preprocessed_data=preprocess_text(data[:1])\n",
    "print(preprocessed_data)\n",
    "\n",
    "# for n,text in enumerate(preprocessed_data):\n",
    "#     for i in  range(1,5):\n",
    "#         if(len(text.split())<i):\n",
    "#             break\n",
    "#         print(i,'-gram:',n_gram(text,i))\n",
    "#         print(i,'Tf-Idf:',tf_idf(preprocessed_data,i)[n])\n",
    "\n",
    "#     print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df_2['Content']\n",
    "print(data[:1])\n",
    "preprocessed_data=preprocess_text(data[:1])\n",
    "print(preprocessed_data)\n",
    "\n",
    "# for n,text in enumerate(preprocessed_data):\n",
    "#     for i in  range(1,5):\n",
    "#         if(len(text.split())<i):\n",
    "#             break\n",
    "#         print(i,'-gram:',n_gram(text,i))\n",
    "#         print(i,'Tf-Idf:',tf_idf(preprocessed_data,i)[n])\n",
    "\n",
    "#     print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
